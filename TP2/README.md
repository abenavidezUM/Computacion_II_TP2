# TP2 - Sistema de Scraping y AnÃ¡lisis Web Distribuido

Sistema distribuido de scraping y anÃ¡lisis web que utiliza programaciÃ³n asÃ­ncrona y paralelismo para extraer y procesar informaciÃ³n de sitios web de forma eficiente.

## DescripciÃ³n

El sistema consta de dos servidores que trabajan de forma coordinada:

- **Servidor de Scraping (Parte A)**: Servidor HTTP asÃ­ncrono que maneja requests de scraping utilizando `asyncio`. Extrae informaciÃ³n estructural de pÃ¡ginas web.
- **Servidor de Procesamiento (Parte B)**: Servidor con `multiprocessing` que ejecuta tareas CPU-intensivas como generaciÃ³n de screenshots, anÃ¡lisis de rendimiento y procesamiento de imÃ¡genes.

## Arquitectura

```
Cliente HTTP
    |
    v
Servidor A (asyncio) ---[socket]---> Servidor B (multiprocessing)
    |                                      |
    |                                      v
    |                                Pool de Workers
    v
Respuesta JSON consolidada
```

## Requisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)

## InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/abenavidezUM/Computacion_II_TP2.git
cd Computacion_II_TP2/TP2
```

### 2. Crear entorno virtual (recomendado)

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. ConfiguraciÃ³n adicional para screenshots

Para la funcionalidad de screenshots, necesitas instalar un driver de navegador:

**OpciÃ³n 1: ChromeDriver (automÃ¡tico con webdriver-manager)**
```bash
# Ya incluido en requirements.txt
```

**OpciÃ³n 2: Playwright (alternativa)**
```bash
pip install playwright
playwright install chromium
```

## Uso

### Iniciar el Servidor de Procesamiento (Servidor B)

Primero, inicia el servidor de procesamiento:

```bash
python server_processing.py -i 127.0.0.1 -p 9000 -n 4
```

**Opciones:**
- `-i, --ip`: DirecciÃ³n IP de escucha
- `-p, --port`: Puerto de escucha
- `-n, --processes`: NÃºmero de procesos en el pool (default: nÃºmero de CPUs)
- `-h, --help`: Muestra ayuda

### Iniciar el Servidor de Scraping (Servidor A)

En otra terminal, inicia el servidor de scraping:

```bash
python server_scraping.py -i 127.0.0.1 -p 8000 -w 4
```

**Opciones:**
- `-i, --ip`: DirecciÃ³n IP de escucha (soporta IPv4/IPv6)
- `-p, --port`: Puerto de escucha
- `-w, --workers`: NÃºmero de workers concurrentes (default: 4)
- `--processor-host`: Host del servidor de procesamiento (default: 127.0.0.1)
- `--processor-port`: Puerto del servidor de procesamiento (default: 9000)
- `-h, --help`: Muestra ayuda

### Usar el Cliente

Realiza un request de scraping:

```bash
python client.py --url https://example.com
```

**Opciones:**
- `--url`: URL del sitio web a scrapear (requerido)
- `--server-host`: Host del servidor de scraping (default: 127.0.0.1)
- `--server-port`: Puerto del servidor de scraping (default: 8000)
- `--timeout`: Timeout en segundos (default: 60)
- `--output`: Archivo para guardar el resultado JSON
- `--process`: Solicitar procesamiento adicional (screenshots, performance, thumbnails)
- `--pretty`: Mostrar formato legible en lugar de JSON
- `--verbose, -v`: InformaciÃ³n detallada

**Ejemplo con todas las opciones:**
```bash
# Simple
python client.py --url https://example.com

# Con formato legible
python client.py --url https://example.com --pretty

# Con procesamiento completo
python client.py --url https://example.com --process --pretty --verbose

# Guardar en archivo
python client.py --url https://github.com --timeout 120 --output result.json
```

### Soporte para IPv6

El servidor de scraping soporta IPv6:

```bash
# Servidor con IPv6
python server_scraping.py -i ::1 -p 8000

# Cliente apuntando a IPv6
python client.py --url https://example.com --server-host ::1
```

## Endpoints del Servidor de Scraping

### GET/POST /scrape

Realiza scraping de una URL.

**ParÃ¡metros:**
- `url` (query parameter): URL a scrapear

**Ejemplo:**
```bash
curl "http://localhost:8000/scrape?url=https://example.com"
```

**Respuesta:**
```json
{
  "url": "https://example.com",
  "timestamp": "2024-11-10T15:30:00Z",
  "scraping_data": {
    "title": "TÃ­tulo de la pÃ¡gina",
    "links": ["url1", "url2"],
    "meta_tags": {
      "description": "...",
      "keywords": "..."
    },
    "structure": {
      "h1": 2,
      "h2": 5
    },
    "images_count": 15
  },
  "processing_data": {
    "screenshot": "base64_encoded_image",
    "performance": {
      "load_time_ms": 1250,
      "total_size_kb": 2048,
      "num_requests": 45
    }
  },
  "status": "success"
}
```

### GET /health

Verifica el estado del servidor.

**Ejemplo:**
```bash
curl "http://localhost:8000/health"
```

## Estructura del Proyecto

```
TP2/
â”œâ”€â”€ server_scraping.py          # Servidor asyncio (Parte A)
â”œâ”€â”€ server_processing.py        # Servidor multiprocessing (Parte B)
â”œâ”€â”€ client.py                   # Cliente de prueba
â”œâ”€â”€ scraper/                    # MÃ³dulo de scraping
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ html_parser.py          # Parsing HTML
â”‚   â”œâ”€â”€ metadata_extractor.py   # ExtracciÃ³n de metadatos
â”‚   â””â”€â”€ async_http.py           # Cliente HTTP asÃ­ncrono
â”œâ”€â”€ processor/                  # MÃ³dulo de procesamiento
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ screenshot.py           # Screenshots
â”‚   â”œâ”€â”€ performance.py          # AnÃ¡lisis de rendimiento
â”‚   â””â”€â”€ image_processor.py      # Procesamiento de imÃ¡genes
â”œâ”€â”€ common/                     # Utilidades comunes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ protocol.py             # Protocolo de comunicaciÃ³n
â”‚   â””â”€â”€ serialization.py        # SerializaciÃ³n de datos
â”œâ”€â”€ tests/                      # Tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â””â”€â”€ test_processor.py
â”œâ”€â”€ requirements.txt            # Dependencias
â””â”€â”€ README.md                   # Este archivo
```

## Testing

El proyecto incluye tests unitarios completos para validar todas las funcionalidades crÃ­ticas.

### Ejecutar Tests

Instalar pytest (si no estÃ¡ instalado):

```bash
pip install pytest
```

Ejecutar todos los tests:

```bash
pytest tests/ -v
```

Ejecutar tests especÃ­ficos:

```bash
# Tests de scraping (HTML parser, metadata extractor)
pytest tests/test_scraper.py -v

# Tests de procesamiento (image processor, validators, limits)
pytest tests/test_processor.py -v
```

### Cobertura de Tests

**test_scraper.py** (18 tests):
- ExtracciÃ³n de tÃ­tulo y fallbacks
- ExtracciÃ³n y conversiÃ³n de enlaces
- ExtracciÃ³n de imÃ¡genes
- AnÃ¡lisis de estructura (H1-H6)
- ExtracciÃ³n de meta tags (basic, Open Graph, Twitter)
- Casos lÃ­mite (HTML malformado, vacÃ­o, sin atributos)

**test_processor.py** (30+ tests):
- GeneraciÃ³n de thumbnails con aspect ratio
- Redimensionamiento de imÃ¡genes
- OptimizaciÃ³n y conversiÃ³n de formatos
- ExtracciÃ³n de informaciÃ³n de imÃ¡genes
- Validadores (URL, puertos, workers, timeouts, dimensiones, calidad, formatos)
- LÃ­mites de seguridad (safe timeouts, quality, dimensions, max images)
- Casos lÃ­mite (datos invÃ¡lidos, valores fuera de rango)

### Tests de IntegraciÃ³n

Scripts de prueba manuales incluidos:

```bash
# Test de comunicaciÃ³n entre servidores
python test_communication.py

# Test de integraciÃ³n completo
python test_integration.py

# Test de performance
python test_performance.py

# Test de imÃ¡genes
python test_images.py
```

## Desarrollo

### Estado Actual

**Etapa 1 - Completada âœ“**
- Estructura de carpetas creada
- CLI implementado para ambos servidores
- Servidores base funcionales
- Cliente de prueba bÃ¡sico

**Etapa 2 - Completada âœ“**
- Servidor HTTP asÃ­ncrono completamente funcional
- Soporte IPv4 e IPv6 verificado
- Middlewares de logging y error handling
- Endpoints /scrape y /health operativos
- ValidaciÃ³n robusta de URLs y parÃ¡metros

**Etapa 3 - Completada âœ“**
- Cliente HTTP asÃ­ncrono con soporte SSL
- Parsing HTML con BeautifulSoup (lxml)
- ExtracciÃ³n de tÃ­tulo, enlaces, meta tags
- AnÃ¡lisis de estructura (H1-H6)
- Conteo y URLs de imÃ¡genes

**Etapa 4 - Completada âœ“**
- Protocolo de comunicaciÃ³n [LENGTH][JSON]
- Servidor TCP con ThreadingTCPServer
- Pool de procesos con multiprocessing
- Handler de tareas con tipos mÃºltiples
- Testing de comunicaciÃ³n exitoso

**Etapa 5 - Completada âœ“**
- IntegraciÃ³n completa Aâ†”B
- Endpoint /scrape con parÃ¡metro ?process=true
- EnvÃ­o automÃ¡tico de tareas al servidor B
- CombinaciÃ³n de resultados en JSON unificado
- Testing end-to-end con 3 tests exitosos

**Etapa 6 - Completada âœ“**
- Screenshots reales con Selenium WebDriver
- Captura full-page y viewport
- TamaÃ±os personalizables (desktop, mobile, etc.)
- Formato PNG en base64
- 3 tests exitosos (example.com, github.com, python.org)

**Etapa 7 - Completada âœ“**
- AnÃ¡lisis de rendimiento con Performance API
- Navigation Timing: DNS, TCP, Request/Response
- Paint Metrics: First Paint, First Contentful Paint
- DOM Metrics: Interactive, Content Loaded, Complete
- Performance Insights y recomendaciones
- Testing directo verificado

**Etapa 8 - Completada âœ“**
- Procesamiento de imÃ¡genes con Pillow (PIL)
- GeneraciÃ³n de thumbnails configurables
- Redimensionamiento y optimizaciÃ³n de imÃ¡genes
- ConversiÃ³n entre formatos (JPEG, PNG, WEBP)
- Descarga sÃ­ncrona de imÃ¡genes (compatible con multiprocessing)
- Procesamiento batch de mÃºltiples imÃ¡genes

**Etapa 9 - Completada âœ“**
- MÃ³dulo de validadores robusto (common/validators.py)
- MÃ³dulo de lÃ­mites de recursos (common/limits.py)
- ValidaciÃ³n completa de URLs y parÃ¡metros
- LÃ­mites de recursos en todas las operaciones
- Documento de cÃ³digos de error (ERROR_CODES.md)
- Manejo de errores estructurado y consistente
- DegradaciÃ³n graciosa ante fallos parciales

**Etapa 10 - Completada âœ“**
- Cliente mejorado con opciones --process, --pretty, --verbose
- Formateo legible de resultados con emojis
- tests/test_scraper.py con 18 tests unitarios
- tests/test_processor.py con 30+ tests unitarios
- Cobertura completa de funcionalidades crÃ­ticas
- DocumentaciÃ³n exhaustiva con ejemplos
- Todos los requisitos del TP cumplidos

**ðŸŽ‰ PROYECTO COMPLETADO - 100% de los requisitos implementados**

**Funcionalidades finales:**
- âœ… Servidor asÃ­ncrono con asyncio + aiohttp
- âœ… Servidor de procesamiento con multiprocessing
- âœ… Protocolo de comunicaciÃ³n inter-servidor personalizado
- âœ… Web scraping completo (HTML, meta tags, estructura)
- âœ… Screenshots reales con Selenium
- âœ… AnÃ¡lisis de rendimiento web (timing, resources, paint)
- âœ… Procesamiento de imÃ¡genes (thumbnails, resize, optimize)
- âœ… ValidaciÃ³n robusta de inputs
- âœ… LÃ­mites de recursos y seguridad
- âœ… Manejo de errores estructurado
- âœ… 48+ tests unitarios
- âœ… DocumentaciÃ³n completa

### Contribuir



## TecnologÃ­as Utilizadas

- **asyncio**: ProgramaciÃ³n asÃ­ncrona
- **aiohttp**: Cliente/servidor HTTP asÃ­ncrono
- **multiprocessing**: Paralelismo con mÃºltiples procesos
- **BeautifulSoup4**: Parsing HTML
- **Selenium**: Screenshots y automation
- **Pillow**: Procesamiento de imÃ¡genes
- **pytest**: Testing

## Licencia

Proyecto acadÃ©mico para ComputaciÃ³n II - Facultad de IngenierÃ­a

## Autores

- AgustÃ­n BenavÃ­dez (@abenavidezUM)
- Legajo: 62344

## Fecha de Entrega

14 de Noviembre de 2025

