# TP2 - Sistema de Scraping y An√°lisis Web Distribuido

Sistema distribuido de scraping y an√°lisis web que utiliza programaci√≥n as√≠ncrona y paralelismo para extraer y procesar informaci√≥n de sitios web de forma eficiente.

## Descripci√≥n

El sistema consta de dos servidores que trabajan de forma coordinada:

- **Servidor de Scraping (Parte A)**: Servidor HTTP as√≠ncrono que maneja requests de scraping utilizando `asyncio`. Extrae informaci√≥n estructural de p√°ginas web.
- **Servidor de Procesamiento (Parte B)**: Servidor con `multiprocessing` que ejecuta tareas CPU-intensivas como generaci√≥n de screenshots, an√°lisis de rendimiento y procesamiento de im√°genes.

## Arquitectura

```
Cliente HTTP
    |
    v
Servidor A (asyncio) ---[socket]---> Servidor B (multiprocessing)
    |                                      |
    |                                      v
    |                                Pool de Workers
    v
Respuesta JSON consolidada
```

## Requisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)

## Instalaci√≥n

### 1. Clonar el repositorio

```bash
git clone https://github.com/abenavidezUM/Computacion_II_TP2.git
cd Computacion_II_TP2/TP2
```

### 2. Crear entorno virtual (recomendado)

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configuraci√≥n adicional para screenshots

Para la funcionalidad de screenshots, necesitas instalar un driver de navegador:

**Opci√≥n 1: ChromeDriver (autom√°tico con webdriver-manager)**
```bash
# Ya incluido en requirements.txt
```

**Opci√≥n 2: Playwright (alternativa)**
```bash
pip install playwright
playwright install chromium
```

## Uso

### Iniciar el Servidor de Procesamiento (Servidor B)

Primero, inicia el servidor de procesamiento:

```bash
python server_processing.py -i 127.0.0.1 -p 9000 -n 4
```

**Opciones:**
- `-i, --ip`: Direcci√≥n IP de escucha
- `-p, --port`: Puerto de escucha
- `-n, --processes`: N√∫mero de procesos en el pool (default: n√∫mero de CPUs)
- `-h, --help`: Muestra ayuda

### Iniciar el Servidor de Scraping (Servidor A)

En otra terminal, inicia el servidor de scraping:

```bash
python server_scraping.py -i 127.0.0.1 -p 8000 -w 4
```

**Opciones:**
- `-i, --ip`: Direcci√≥n IP de escucha (soporta IPv4/IPv6)
- `-p, --port`: Puerto de escucha
- `-w, --workers`: N√∫mero de workers concurrentes (default: 4)
- `--processor-host`: Host del servidor de procesamiento (default: 127.0.0.1)
- `--processor-port`: Puerto del servidor de procesamiento (default: 9000)
- `-h, --help`: Muestra ayuda

### Usar el Cliente

Realiza un request de scraping:

```bash
python client.py --url https://example.com
```

**Opciones:**
- `--url`: URL del sitio web a scrapear (requerido)
- `--server-host`: Host del servidor de scraping (default: 127.0.0.1)
- `--server-port`: Puerto del servidor de scraping (default: 8000)
- `--timeout`: Timeout en segundos (default: 60)
- `--output`: Archivo para guardar el resultado JSON
- `--process`: Solicitar procesamiento adicional (screenshots, performance, thumbnails)
- `--pretty`: Mostrar formato legible en lugar de JSON
- `--verbose, -v`: Informaci√≥n detallada

**Ejemplo con todas las opciones:**
```bash
# Simple
python client.py --url https://example.com

# Con formato legible
python client.py --url https://example.com --pretty

# Con procesamiento completo
python client.py --url https://example.com --process --pretty --verbose

# Guardar en archivo
python client.py --url https://github.com --timeout 120 --output result.json
```

### Soporte para IPv6

El servidor de scraping soporta IPv6:

```bash
# Servidor con IPv6
python server_scraping.py -i ::1 -p 8000

# Cliente apuntando a IPv6
python client.py --url https://example.com --server-host ::1
```

## Endpoints del Servidor de Scraping

### GET/POST /scrape (Modo S√≠ncrono)

Realiza scraping de una URL y espera el resultado.

**Par√°metros:**
- `url` (query parameter): URL a scrapear

**Ejemplo:**
```bash
curl "http://localhost:8000/scrape?url=https://example.com"
```

**Respuesta:**
```json
{
  "url": "https://example.com",
  "timestamp": "2024-11-10T15:30:00Z",
  "scraping_data": {
    "title": "T√≠tulo de la p√°gina",
    "links": ["url1", "url2"],
    "meta_tags": {
      "description": "...",
      "keywords": "..."
    },
    "structure": {
      "h1": 2,
      "h2": 5
    },
    "images_count": 15
  },
  "processing_data": {
    "screenshot": "base64_encoded_image",
    "performance": {
      "load_time_ms": 1250,
      "total_size_kb": 2048,
      "num_requests": 45
    }
  },
  "status": "success"
}
```

### GET /health

Verifica el estado del servidor.

**Ejemplo:**
```bash
curl "http://localhost:8000/health"
```

---

## üéÅ Bonus Track - Sistema de Tareas As√≠ncronas (Etapa 11)

El servidor incluye un sistema de tareas as√≠ncronas que permite:
- **Respuestas inmediatas** sin esperar a que termine el scraping
- **Consulta de estado** en tiempo real
- **Procesamiento en background** con workers as√≠ncronos
- **M√∫ltiples tareas en paralelo** sin bloqueos

### GET/POST /scrape/async

Crea una tarea de scraping as√≠ncrona y retorna inmediatamente un `task_id`.

**Par√°metros:**
- `url` (query parameter): URL a scrapear
- `process` (optional): Si "true", incluye procesamiento adicional

**Respuesta (HTTP 202):**
```json
{
  "status": "success",
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Task created successfully",
  "url": "https://example.com",
  "process": false,
  "endpoints": {
    "status": "/status/550e8400-e29b-41d4-a716-446655440000",
    "result": "/result/550e8400-e29b-41d4-a716-446655440000"
  }
}
```

**Ejemplo:**
```bash
# Crear tarea as√≠ncrona
curl -X POST "http://localhost:8000/scrape/async?url=https://example.com"

# Con procesamiento completo
curl -X POST "http://localhost:8000/scrape/async?url=https://example.com&process=true"
```

### GET /status/{task_id}

Consulta el estado de una tarea.

**Estados posibles:**
- `pending`: Tarea en cola, esperando procesamiento
- `processing`: Tarea siendo procesada actualmente
- `completed`: Tarea completada exitosamente
- `failed`: Tarea fall√≥ con error

**Respuesta:**
```json
{
  "status": "success",
  "task": {
    "task_id": "550e8400-e29b-41d4-a716-446655440000",
    "url": "https://example.com",
    "status": "processing",
    "created_at": "2025-11-07T17:30:00.000000",
    "started_at": "2025-11-07T17:30:01.000000",
    "completed_at": null,
    "has_result": false,
    "error": null
  }
}
```

**Ejemplo:**
```bash
curl "http://localhost:8000/status/550e8400-e29b-41d4-a716-446655440000"
```

### GET /result/{task_id}

Obtiene el resultado de una tarea completada.

**Respuesta si est√° pending/processing (HTTP 202):**
```json
{
  "status": "processing",
  "message": "Task is being processed"
}
```

**Respuesta si est√° completed (HTTP 200):**
```json
{
  "url": "https://example.com",
  "timestamp": "2025-11-07T17:30:05.000000",
  "status": "success",
  "scraping_data": {
    "title": "Example Domain",
    "links_count": 1,
    "images_count": 0,
    ...
  }
}
```

**Ejemplo:**
```bash
curl "http://localhost:8000/result/550e8400-e29b-41d4-a716-446655440000"
```

### GET /stats

Obtiene estad√≠sticas del servidor de tareas.

**Respuesta:**
```json
{
  "status": "success",
  "stats": {
    "total_tasks": 15,
    "pending": 2,
    "processing": 1,
    "completed": 10,
    "failed": 2,
    "max_tasks": 1000
  }
}
```

**Ejemplo:**
```bash
curl "http://localhost:8000/stats"
```

### Flujo de Trabajo con Tareas As√≠ncronas

```bash
# 1. Crear tarea (respuesta inmediata)
TASK_ID=$(curl -s -X POST "http://localhost:8000/scrape/async?url=https://example.com" | jq -r '.task_id')

# 2. Consultar estado (puede hacer polling)
curl "http://localhost:8000/status/$TASK_ID"

# 3. Esperar y obtener resultado cuando est√© listo
curl "http://localhost:8000/result/$TASK_ID"
```

### Test del Sistema de Tareas

Ejecutar el script de prueba incluido:

```bash
python test_async_tasks.py
```

Este script prueba:
- Creaci√≥n de tareas as√≠ncronas
- Consulta de estado en diferentes momentos
- Obtenci√≥n de resultados
- Creaci√≥n de m√∫ltiples tareas en paralelo
- Estad√≠sticas del servidor
- Manejo de errores (tareas inexistentes)

---

## Estructura del Proyecto

```
TP2/
‚îú‚îÄ‚îÄ server_scraping.py          # Servidor asyncio (Parte A)
‚îú‚îÄ‚îÄ server_processing.py        # Servidor multiprocessing (Parte B)
‚îú‚îÄ‚îÄ client.py                   # Cliente de prueba
‚îú‚îÄ‚îÄ scraper/                    # M√≥dulo de scraping
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ html_parser.py          # Parsing HTML
‚îÇ   ‚îú‚îÄ‚îÄ metadata_extractor.py   # Extracci√≥n de metadatos
‚îÇ   ‚îî‚îÄ‚îÄ async_http.py           # Cliente HTTP as√≠ncrono
‚îú‚îÄ‚îÄ processor/                  # M√≥dulo de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ screenshot.py           # Screenshots
‚îÇ   ‚îú‚îÄ‚îÄ performance.py          # An√°lisis de rendimiento
‚îÇ   ‚îî‚îÄ‚îÄ image_processor.py      # Procesamiento de im√°genes
‚îú‚îÄ‚îÄ common/                     # Utilidades comunes
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ protocol.py             # Protocolo de comunicaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ serialization.py        # Serializaci√≥n de datos
‚îú‚îÄ‚îÄ tests/                      # Tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_scraper.py
‚îÇ   ‚îî‚îÄ‚îÄ test_processor.py
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias
‚îî‚îÄ‚îÄ README.md                   # Este archivo
```

## Testing

El proyecto incluye tests unitarios completos para validar todas las funcionalidades cr√≠ticas.

### Ejecutar Tests

Instalar pytest (si no est√° instalado):

```bash
pip install pytest
```

Ejecutar todos los tests:

```bash
pytest tests/ -v
```

Ejecutar tests espec√≠ficos:

```bash
# Tests de scraping (HTML parser, metadata extractor)
pytest tests/test_scraper.py -v

# Tests de procesamiento (image processor, validators, limits)
pytest tests/test_processor.py -v
```

### Cobertura de Tests

**test_scraper.py** (18 tests):
- Extracci√≥n de t√≠tulo y fallbacks
- Extracci√≥n y conversi√≥n de enlaces
- Extracci√≥n de im√°genes
- An√°lisis de estructura (H1-H6)
- Extracci√≥n de meta tags (basic, Open Graph, Twitter)
- Casos l√≠mite (HTML malformado, vac√≠o, sin atributos)

**test_processor.py** (30+ tests):
- Generaci√≥n de thumbnails con aspect ratio
- Redimensionamiento de im√°genes
- Optimizaci√≥n y conversi√≥n de formatos
- Extracci√≥n de informaci√≥n de im√°genes
- Validadores (URL, puertos, workers, timeouts, dimensiones, calidad, formatos)
- L√≠mites de seguridad (safe timeouts, quality, dimensions, max images)
- Casos l√≠mite (datos inv√°lidos, valores fuera de rango)

### Tests de Integraci√≥n

Scripts de prueba manuales incluidos:

```bash
# Test de comunicaci√≥n entre servidores
python test_communication.py

# Test de integraci√≥n completo
python test_integration.py

# Test de performance
python test_performance.py

# Test de im√°genes
python test_images.py
```

## Desarrollo

### Estado Actual

**Etapa 1 - Completada ‚úì**
- Estructura de carpetas creada
- CLI implementado para ambos servidores
- Servidores base funcionales
- Cliente de prueba b√°sico

**Etapa 2 - Completada ‚úì**
- Servidor HTTP as√≠ncrono completamente funcional
- Soporte IPv4 e IPv6 verificado
- Middlewares de logging y error handling
- Endpoints /scrape y /health operativos
- Validaci√≥n robusta de URLs y par√°metros

**Etapa 3 - Completada ‚úì**
- Cliente HTTP as√≠ncrono con soporte SSL
- Parsing HTML con BeautifulSoup (lxml)
- Extracci√≥n de t√≠tulo, enlaces, meta tags
- An√°lisis de estructura (H1-H6)
- Conteo y URLs de im√°genes

**Etapa 4 - Completada ‚úì**
- Protocolo de comunicaci√≥n [LENGTH][JSON]
- Servidor TCP con ThreadingTCPServer
- Pool de procesos con multiprocessing
- Handler de tareas con tipos m√∫ltiples
- Testing de comunicaci√≥n exitoso

**Etapa 5 - Completada ‚úì**
- Integraci√≥n completa A‚ÜîB
- Endpoint /scrape con par√°metro ?process=true
- Env√≠o autom√°tico de tareas al servidor B
- Combinaci√≥n de resultados en JSON unificado
- Testing end-to-end con 3 tests exitosos

**Etapa 6 - Completada ‚úì**
- Screenshots reales con Selenium WebDriver
- Captura full-page y viewport
- Tama√±os personalizables (desktop, mobile, etc.)
- Formato PNG en base64
- 3 tests exitosos (example.com, github.com, python.org)

**Etapa 7 - Completada ‚úì**
- An√°lisis de rendimiento con Performance API
- Navigation Timing: DNS, TCP, Request/Response
- Paint Metrics: First Paint, First Contentful Paint
- DOM Metrics: Interactive, Content Loaded, Complete
- Performance Insights y recomendaciones
- Testing directo verificado

**Etapa 8 - Completada ‚úì**
- Procesamiento de im√°genes con Pillow (PIL)
- Generaci√≥n de thumbnails configurables
- Redimensionamiento y optimizaci√≥n de im√°genes
- Conversi√≥n entre formatos (JPEG, PNG, WEBP)
- Descarga s√≠ncrona de im√°genes (compatible con multiprocessing)
- Procesamiento batch de m√∫ltiples im√°genes

**Etapa 9 - Completada ‚úì**
- M√≥dulo de validadores robusto (common/validators.py)
- M√≥dulo de l√≠mites de recursos (common/limits.py)
- Validaci√≥n completa de URLs y par√°metros
- L√≠mites de recursos en todas las operaciones
- Documento de c√≥digos de error (ERROR_CODES.md)
- Manejo de errores estructurado y consistente
- Degradaci√≥n graciosa ante fallos parciales

**Etapa 10 - Completada ‚úì**
- Cliente mejorado con opciones --process, --pretty, --verbose
- Formateo legible de resultados con emojis
- tests/test_scraper.py con 18 tests unitarios
- tests/test_processor.py con 30+ tests unitarios
- Cobertura completa de funcionalidades cr√≠ticas
- Documentaci√≥n exhaustiva con ejemplos
- Todos los requisitos del TP cumplidos

**üéâ PROYECTO COMPLETADO - 100% de los requisitos + Bonus Track**

**Etapa 11 - Completada ‚úì (Bonus Track)**
- Sistema de cola con task IDs (UUIDs √∫nicos)
- Respuestas inmediatas sin esperar (HTTP 202)
- Endpoint POST /scrape/async para crear tareas
- Endpoint GET /status/{task_id} para consultar estado
- Endpoint GET /result/{task_id} para obtener resultado
- Endpoint GET /stats para estad√≠sticas del servidor
- Estados: pending, processing, completed, failed
- TaskManager con hasta 1000 tareas en memoria
- Worker as√≠ncrono procesando en background
- Script test_async_tasks.py con 9 tests
- Cleanup autom√°tico de tareas antiguas (FIFO)
- Timestamps completos (creaci√≥n, inicio, finalizaci√≥n)

**Funcionalidades finales:**
- ‚úÖ Servidor as√≠ncrono con asyncio + aiohttp
- ‚úÖ Servidor de procesamiento con multiprocessing
- ‚úÖ Protocolo de comunicaci√≥n inter-servidor personalizado
- ‚úÖ Web scraping completo (HTML, meta tags, estructura)
- ‚úÖ Screenshots reales con Selenium
- ‚úÖ An√°lisis de rendimiento web (timing, resources, paint)
- ‚úÖ Procesamiento de im√°genes (thumbnails, resize, optimize)
- ‚úÖ Validaci√≥n robusta de inputs
- ‚úÖ L√≠mites de recursos y seguridad
- ‚úÖ Manejo de errores estructurado
- ‚úÖ 48+ tests unitarios
- ‚úÖ Documentaci√≥n completa

### Contribuir



## Tecnolog√≠as Utilizadas

- **asyncio**: Programaci√≥n as√≠ncrona
- **aiohttp**: Cliente/servidor HTTP as√≠ncrono
- **multiprocessing**: Paralelismo con m√∫ltiples procesos
- **BeautifulSoup4**: Parsing HTML
- **Selenium**: Screenshots y automation
- **Pillow**: Procesamiento de im√°genes
- **pytest**: Testing

## Licencia

Proyecto acad√©mico para Computaci√≥n II - Facultad de Ingenier√≠a

## Autores

- Agust√≠n Benav√≠dez (@abenavidezUM)
- Legajo: 62344

## Fecha de Entrega

14 de Noviembre de 2025

